{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebb38c6",
   "metadata": {},
   "source": [
    "# TD Learning: SARSA e *Q*-learning\n",
    "\n",
    "#### Prof. Armando Alves Neto - Introdução ao Aprendizado por Reforço - PPGEE/UFMG\n",
    "\n",
    "Objetivo: ensinar um robô a navegar através de um labirinto até um ponto-alvo específico.\n",
    "\n",
    "<img src=\"problema_labirinto.png\" width=\"300\">\n",
    "\n",
    "## Características do labirinto:\n",
    "\n",
    "### Espaço de observações\n",
    "\n",
    "O labirinto corresponde a um espaço de 10x10 metros, discretizado em um grid de 25x25.\n",
    "\n",
    "### Espaço de ações\n",
    "\n",
    "O robô pode dar um passo em todas as 8 direções (todos os vizinhos são alcançáveis), ou pode ficar parado.\n",
    "\n",
    "### Função de recompensa\n",
    "\n",
    "- Se alcançar o objetivo, recebe +100\n",
    "- Se o número de passo exceder 100, recebe -20\n",
    "- Se o robô colidir com algum obstáculo, recebe -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except:\n",
    "    import gym\n",
    "from functools import partial\n",
    "import class_maze as cm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16,8)\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59746e67",
   "metadata": {},
   "source": [
    "Criando a classe para o TD learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d3a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TDlearning(object):\n",
    "    def __init__(self, parameters):\n",
    "\n",
    "        self.parameters = parameters\n",
    "\n",
    "        # metodo\n",
    "        self.method = parameters['method']\n",
    "\n",
    "        # numero de episodios\n",
    "        self.episode = 0\n",
    "\n",
    "        # cria o ambiente\n",
    "        self.env = cm.Maze()\n",
    "\n",
    "        # tamanho dos espacos de estados e acoes\n",
    "        self.num_states = np.prod(np.array(self.env.num_states))\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # parametros de aprendizado\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.eps = parameters['eps']\n",
    "        self.alpha = parameters['alpha']\n",
    "\n",
    "        # log file (name depends on the method)\n",
    "        self.logfile = parameters['q-file']\n",
    "        if self.method == 'SARSA':\n",
    "            self.logfile = 'sarsa_' + self.logfile\n",
    "        elif self.method == 'Q-learning':\n",
    "            self.logfile = 'qlearning_' + self.logfile\n",
    "        else: print(\"Não salvou...\")\n",
    "\n",
    "        # reseta a politica\n",
    "        self.reset()\n",
    "\n",
    "    ##########################################\n",
    "    # reseta a funcao acao-valor\n",
    "    def reset(self):\n",
    "        \n",
    "        # reseta o ambiente\n",
    "        S = self.env.reset()\n",
    "        \n",
    "        # Q(s,a)\n",
    "        self.Q = np.zeros((self.num_states, self.num_actions))\n",
    "\n",
    "        # carrega tabela pre-computada se for o caso\n",
    "        if self.parameters['load_Q']:\n",
    "            try:\n",
    "                with open(self.logfile, 'rb') as f:\n",
    "                    data = np.load(f)\n",
    "                    self.Q = data['Q']\n",
    "                    self.episode = data['episodes']\n",
    "            except: None\n",
    "\n",
    "    ##########################################\n",
    "    # retorna a politica corrente\n",
    "    def curr_policy(self, copy=False):\n",
    "        if copy:\n",
    "            return partial(self.TabularEpsilonGreedyPolicy, np.copy(self.Q))\n",
    "        else:\n",
    "            return partial(self.TabularEpsilonGreedyPolicy, self.Q)\n",
    "        \n",
    "    ########################################\n",
    "    # salva tabela Q(s,a)\n",
    "    def save(self):\n",
    "        with open(self.logfile, 'wb') as f:\n",
    "            np.savez(f, Q=self.Q, episodes=self.episode)\n",
    "\n",
    "    ##########################################\n",
    "    def __del__(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4321853",
   "metadata": {},
   "source": [
    "Probabilidade de escolha de uma ação $a$ baseada na política $\\varepsilon$-soft:\n",
    "$$\n",
    "\\pi(a|S_t) \\gets \n",
    "                        \\begin{cases}\n",
    "                            1 - \\varepsilon + \\varepsilon/|\\mathcal{A}|,  & \\text{se}~ a = \\arg\\max\\limits_{a} Q(S_t,a),\\\\\n",
    "                            \\varepsilon/|\\mathcal{A}|, & \\text{caso contrário.}\n",
    "                        \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5807b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    # escolha da açao (epsilon-soft)\n",
    "    def TabularEpsilonGreedyPolicy(self, Q, state):\n",
    "\n",
    "        # acao otima corrente\n",
    "        Aast = Q[state, :].argmax()\n",
    "\n",
    "        # numero total de acoes\n",
    "        nactions = Q.shape[1]\n",
    "    \n",
    "        # probabilidades de escolher as acoes\n",
    "        p1 = 1.0 - self.eps + self.eps/nactions\n",
    "        p2 = self.eps/nactions\n",
    "        prob = [p1 if a == Aast else p2 for a in range(nactions)]\n",
    "        \n",
    "        return np.random.choice(nactions, p=np.array(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c550a0",
   "metadata": {},
   "source": [
    "Método do SARSA:\n",
    "- aplique ação $A$, receba $S'$ e $R$\n",
    "- escolha $A'$ a partir de $S'$ usando $Q$ ($\\varepsilon$-greedy, por exemplo)\n",
    "- $Q(S,A) \\gets Q(S,A) + \\alpha \\big[R + \\gamma Q(S',A') - Q(S,A)\\big]$\n",
    "- $S \\gets S'$\n",
    "- $A \\gets A'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6515a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    def sarsa(self, S, A):\n",
    "\n",
    "        # passo de interacao com o ambiente\n",
    "        [Sl, R, done, _] = self.env.step(A)\n",
    "        \n",
    "        # escolhe A' a partir de S'\n",
    "        Al = self.policy(Sl)\n",
    "        \n",
    "        # update de Q(s,a)\n",
    "        self.Q[S, A] = self.Q[S, A] + self.alpha*(R + self.gamma*self.Q[Sl, Al] - self.Q[S, A])\n",
    "        \n",
    "        return Sl, Al, R, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902209ce",
   "metadata": {},
   "source": [
    "Método do *Q*-learning:\n",
    "- escolha $A$ a partir de $S$ usando $Q$ ($\\varepsilon$-greedy, por exemplo)\n",
    "- aplique ação $A$, receba $S'$ e $R$\n",
    "- $Q(S,A) \\gets Q(S,A) + \\alpha \\big[R + \\gamma \\max\\limits_a Q(S',a) - Q(S,A)\\big]$\n",
    "- $S \\gets S'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb859fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    def qlearning(self, S):\n",
    "        \n",
    "        # \\pi(s)\n",
    "        A = self.policy(S)\n",
    "\n",
    "        # passo de interacao com o ambiente\n",
    "        [Sl, R, done, _] = self.env.step(A)\n",
    "        \n",
    "        self.Q[S, A] = self.Q[S, A] + self.alpha*(R + self.gamma*self.Q[Sl, :].max() - self.Q[S, A])\n",
    "        \n",
    "        return Sl, R, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f936cf",
   "metadata": {},
   "source": [
    "Executando um dos dois métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf6614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    # simula um episodio até o fim seguindo a politica corente\n",
    "    def rollout(self, max_iter=1000, render=False):\n",
    "        \n",
    "        # inicia o ambiente (começa aleatoriamente)\n",
    "        S = self.env.reset()\n",
    "        \n",
    "        # \\pi(s)\n",
    "        A = self.policy(S)\n",
    "\n",
    "        # lista de rewards\n",
    "        rewards = []\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            \n",
    "            if self.method == 'SARSA':\n",
    "                Sl, Al, R, done = self.sarsa(S, A)\n",
    "                # proximo estado e ação\n",
    "                S = Sl\n",
    "                A = Al\n",
    "                \n",
    "            elif self.method == 'Q-learning':\n",
    "                Sl, R, done = self.qlearning(S)\n",
    "                # proximo estado\n",
    "                S = Sl\n",
    "\n",
    "            # Salva rewards\n",
    "            rewards.append(R)\n",
    "\n",
    "            # renderiza o ambiente\n",
    "            if render:\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.gca().clear()\n",
    "                self.env.render(self.Q)\n",
    "\n",
    "            # chegou a um estado terminal?\n",
    "            if done: break\n",
    "\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6f8dc",
   "metadata": {},
   "source": [
    "Executando um episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a07fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    def runEpisode(self):\n",
    "\n",
    "        # novo episodio\n",
    "        self.episode += 1\n",
    "\n",
    "        # pega a politica corrente (on-policy)\n",
    "        self.policy = self.curr_policy()\n",
    "\n",
    "        # gera um episodio seguindo a politica corrente\n",
    "        rewards = self.rollout(render=((self.episode-1)%100 == 0))\n",
    "        \n",
    "        # salva a tabela Q\n",
    "        if self.parameters['save_Q']:\n",
    "            self.save()\n",
    "\n",
    "        return np.sum(np.array(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9812acd",
   "metadata": {},
   "source": [
    "Código principal:\n",
    "- episodes: número de episódios\n",
    "- gamma: fator de desconto\n",
    "- eps: $\\varepsilon$\n",
    "- alpha: $\\alpha$\n",
    "- method: *SARSA* ou *Q-learning*\n",
    "- save_Q: salva tabela *Q*\n",
    "- load_Q: carrega tabela *Q*\n",
    "- q-file: arquivo da tabela *Q*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    plt.ion()\n",
    "\n",
    "    # parametros\n",
    "    parameters = {'episodes'  : 2000,\n",
    "                  'gamma'     : 0.99,\n",
    "                  'eps'       : 1.0e-2,\n",
    "                  'alpha'     : 0.5,\n",
    "                  'method'    : 'SARSA', #'SARSA' ou 'Q-learning'\n",
    "                  'save_Q'    : True,\n",
    "                  'load_Q'    : False,\n",
    "                  'q-file'    : 'qtable.npy',}\n",
    "\n",
    "    # TD algorithm\n",
    "    mc = TDlearning(parameters)\n",
    "\n",
    "    # historico de recompensas\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.gcf().tight_layout()\n",
    "    \n",
    "    while mc.episode <= parameters['episodes']:\n",
    "        # roda um episodio\n",
    "        total_reward = mc.runEpisode()\n",
    "        \n",
    "        # rewrds\n",
    "        rewards.append(total_reward)\n",
    "        # reward medio\n",
    "        avg_rewards.append(np.mean(rewards[-50:]))\n",
    "        \n",
    "        # plot rewards\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.gca().clear()\n",
    "        plt.gca().set_box_aspect(.5)\n",
    "        plt.title('Recompensa por episódios')\n",
    "        plt.plot(avg_rewards, 'b', linewidth=2)\n",
    "        plt.plot(rewards, 'r', alpha=0.3)\n",
    "        plt.xlabel('Episódios')\n",
    "        plt.ylabel('Recompensa')\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(.1)\n",
    "\n",
    "    plt.ioff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
