{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73efbc5b",
   "metadata": {},
   "source": [
    "# TD Learning: Sarsa e *Q*-learning\n",
    "\n",
    "#### Prof. Armando Alves Neto - Introdução ao Aprendizado por Reforço - PPGEE/UFMG\n",
    "\n",
    "<img src=\"cave.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d3b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import gym\n",
    "from functools import partial\n",
    "import class_maze as cm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,5)\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360de7dc",
   "metadata": {},
   "source": [
    "Cria uma classe (```RunningAverage()```) apenas para calcular a média móvel do sinal de reforço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452ae53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage(object):\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.vals = []\n",
    "        self.num_filled = 0\n",
    "\n",
    "    def push(self, val):\n",
    "        if self.num_filled == self.N:\n",
    "            self.vals.pop(0)\n",
    "            self.vals.append(val)\n",
    "        else:\n",
    "            self.vals.append(val)\n",
    "            self.num_filled += 1\n",
    "\n",
    "    def get(self):\n",
    "        return float(sum(self.vals)) / self.num_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59746e67",
   "metadata": {},
   "source": [
    "Criando a classe para o TD learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9d3a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TDlearning(object):\n",
    "    def __init__(self, parameters):\n",
    "\n",
    "        self.parameters = parameters\n",
    "\n",
    "        # metodo\n",
    "        self.method = parameters['method']\n",
    "\n",
    "        # numero de episodios\n",
    "        self.episode = 0\n",
    "\n",
    "        # cria o ambiente\n",
    "        xlim = np.array([0.0, 10.0])\n",
    "        ylim = np.array([0.0, 10.0])\n",
    "        resolution = 0.4\n",
    "        self.env = cm.Maze(xlim=xlim, ylim=ylim, res=resolution, image='cave.png')\n",
    "\n",
    "        # tamanho dos espacos de estados e acoes\n",
    "        self.num_states = np.prod(np.array(self.env.num_states))\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # parametros de aprendizado\n",
    "        self.gamma = parameters['gamma']\n",
    "        self.eps = parameters['eps']\n",
    "        self.alpha = parameters['alpha']\n",
    "\n",
    "        # log file\n",
    "        self.logfile = parameters['q-file']\n",
    "\n",
    "        # reseta a politica\n",
    "        self.reset_policy()\n",
    "\n",
    "    ##########################################\n",
    "    # reseta a funcao acao-valor\n",
    "    def reset_policy(self):\n",
    "        \n",
    "        # Q(s,a)\n",
    "        self.Q = np.zeros((self.num_states, self.num_actions))\n",
    "\n",
    "        if self.parameters['load_Q']:\n",
    "            try:\n",
    "                with open(self.logfile, 'rb') as f:\n",
    "                    data = np.load(f)\n",
    "                    self.Q = data['Q']\n",
    "                    self.episode = data['episodes']\n",
    "            except: None\n",
    "\n",
    "    ##########################################\n",
    "    # retorna a politica corrente\n",
    "    def curr_policy(self, copy=False):\n",
    "        if copy:\n",
    "            return partial(self.TabularEpsilonGreedyPolicy, np.copy(self.Q))\n",
    "        else:\n",
    "            return partial(self.TabularEpsilonGreedyPolicy, self.Q)\n",
    "        \n",
    "    ########################################\n",
    "    # salva tabela Q(s,a)\n",
    "    def save(self):\n",
    "        with open(self.logfile, 'wb') as f:\n",
    "            np.savez(f, Q=self.Q, episodes=self.episode)\n",
    "\n",
    "    ##########################################\n",
    "    def __del__(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4321853",
   "metadata": {},
   "source": [
    "Probabilidade de escolha de uma ação $a$ baseada na política $\\varepsilon$-soft:\n",
    "$$\n",
    "\\pi(a|S_t) \\gets \n",
    "                        \\begin{cases}\n",
    "                            1 - \\varepsilon + \\varepsilon/|\\mathcal{A}|,  & \\text{se}~ a = A^*,\\\\\n",
    "                            \\varepsilon/|\\mathcal{A}|, & \\text{caso contrário.}\n",
    "                        \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5807b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    # escolha da açao (epsilon-soft)\n",
    "    def TabularEpsilonGreedyPolicy(self, Q, state):\n",
    "\n",
    "        # acao otima corrente\n",
    "        Aast = Q[state, :].argmax()\n",
    "\n",
    "        # numero total de acoes\n",
    "        nactions = Q.shape[1]\n",
    "    \n",
    "        # probabilidades de escolher as acoes\n",
    "        p1 = 1.0 - self.eps + self.eps/nactions\n",
    "        p2 = self.eps/nactions\n",
    "        prob = [p1 if a == Aast else p2 for a in range(nactions)]\n",
    "        \n",
    "        return np.random.choice(nactions, p=np.array(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c550a0",
   "metadata": {},
   "source": [
    "Método do Sarsa:\n",
    "- aplique ação $A$, receba $S'$ e $R$\n",
    "- escolha $A'$ a partir de $S'$ usando $Q$ ($\\varepsilon$-greedy, por exemplo)\n",
    "- $Q(S,A) \\gets Q(S,A) + \\alpha \\big[R + \\gamma Q(S',A') - Q(S,A)\\big]$\n",
    "- $S \\gets S'$\n",
    "- $A \\gets A'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b844b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    def sarsa(self, S, A):\n",
    "\n",
    "        # passo de interacao com o ambiente\n",
    "        [Sl, R, done, _] = self.env.step(A)\n",
    "        \n",
    "        # escolhe A' a partir de S'\n",
    "        Al = self.policy(Sl)\n",
    "        \n",
    "        # update de Q(s,a)\n",
    "        self.Q[S, A] = self.Q[S, A] + self.alpha*(R + self.gamma*self.Q[Sl, Al] - self.Q[S, A])\n",
    "        \n",
    "        return Sl, Al, R, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b297f",
   "metadata": {},
   "source": [
    "Método do *Q*-learning:\n",
    "- escolha $A$ a partir de $S$ usando $Q$ ($\\varepsilon$-greedy, por exemplo)\n",
    "- aplique ação $A$, receba $S'$ e $R$\n",
    "- $Q(S,A) \\gets Q(S,A) + \\alpha \\big[R + \\gamma \\max\\limits_a Q(S',a) - Q(S,A)\\big]$\n",
    "- $S \\gets S'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e89d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    def qlearning(self, S):\n",
    "        \n",
    "        # \\pi(s)\n",
    "        A = self.policy(S)\n",
    "\n",
    "        # passo de interacao com o ambiente\n",
    "        [Sl, R, done, _] = self.env.step(A)\n",
    "        \n",
    "        self.Q[S, A] = self.Q[S, A] + self.alpha*(R + self.gamma*self.Q[Sl, :].max() - self.Q[S, A])\n",
    "        \n",
    "        return Sl, R, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d02a9c",
   "metadata": {},
   "source": [
    "Executando um dos dois métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf6614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    # simula um episodio até o fim seguindo a politica corente\n",
    "    def rollout(self, max_iter=1000, render=False):\n",
    "\n",
    "        # inicia o ambiente (começa aleatoriamente)\n",
    "        S = self.env.reset()\n",
    "        \n",
    "        # \\pi(s)\n",
    "        A = self.policy(S)\n",
    "\n",
    "        # lista de rewards\n",
    "        rewards = []\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            \n",
    "            if self.method == 'Sarsa':\n",
    "                Sl, Al, R, done = self.sarsa(S, A)\n",
    "                # proximo estado e ação\n",
    "                S = Sl\n",
    "                A = Al\n",
    "                \n",
    "            elif self.method == 'Q-learning':\n",
    "                Sl, R, done = self.qlearning(S)\n",
    "                # proximo estado\n",
    "                S = Sl\n",
    "\n",
    "            # Salva rewards\n",
    "            rewards.append(R)\n",
    "\n",
    "            # renderiza o ambiente\n",
    "            if render:\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.gca().clear()\n",
    "                self.env.render(self.Q)\n",
    "\n",
    "            # chegou a um estado terminal?\n",
    "            if done: break\n",
    "\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf787b1",
   "metadata": {},
   "source": [
    "Executando um episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a07fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDlearning(TDlearning):\n",
    "    ##########################################\n",
    "    def runEpisode(self):\n",
    "\n",
    "        # novo episodio\n",
    "        self.episode += 1\n",
    "\n",
    "        # pega a politica corrente (on-policy)\n",
    "        self.policy = self.curr_policy()\n",
    "\n",
    "        # gera um episodio seguindo a politica corrente\n",
    "        rewards = self.rollout(render=((self.episode-1)%100 == 0))\n",
    "        \n",
    "        # salva a tabela Q\n",
    "        if self.parameters['save_Q']:\n",
    "            self.save()\n",
    "\n",
    "        return np.sum(np.array(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9812acd",
   "metadata": {},
   "source": [
    "Código principal:\n",
    "- episodes: número de episódios\n",
    "- gamma: fator de desconto\n",
    "- eps: $\\varepsilon$\n",
    "- alpha: $\\alpha$\n",
    "- method: *Sarsa* ou *Q-learning*\n",
    "- save_Q: salva tabela *Q*\n",
    "- load_Q: carrega tabela *Q*\n",
    "- q-file: arquivo da tabela *Q*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4516f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# main\n",
    "##########################################\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    plt.ion()\n",
    "    \n",
    "    # cria objeto para calculo da média movel do reward\n",
    "    avg_calc = RunningAverage(100)\n",
    "\n",
    "    # parametros\n",
    "    parameters = {'episodes'  : 2000,\n",
    "                  'gamma'     : 0.99,\n",
    "                  'eps'       : 0.5e-2,\n",
    "                  'alpha'     : 0.5,\n",
    "                  'method'    : 'Q-learning', #'Sarsa' ou 'Q-learning'\n",
    "                  'save_Q'    : True,\n",
    "                  'load_Q'    : False,\n",
    "                  'q-file'    : 'q-table.npy',}\n",
    "\n",
    "    # TD algorithm\n",
    "    mc = TDlearning(parameters)\n",
    "\n",
    "    # historico dos reforços\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "\n",
    "    while mc.episode <= parameters['episodes']:\n",
    "        # roda um episodio\n",
    "        total_reward = mc.runEpisode()\n",
    "        \n",
    "        # rewrds\n",
    "        rewards.append(total_reward)\n",
    "        # reward medio\n",
    "        avg_calc.push(total_reward)\n",
    "        avg_rewards.append(avg_calc.get())\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.subplot(1, 3, (1,2))\n",
    "        plt.gca().clear()\n",
    "        plt.plot(avg_rewards, 'b', linewidth=2)\n",
    "        plt.plot(rewards, 'r', alpha=0.3)\n",
    "        plt.title('Reforço por episódios')\n",
    "        plt.xlabel('Episódios')\n",
    "        plt.ylabel('Reforço')\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(.1)\n",
    "\n",
    "    plt.ioff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
